{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6209f0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from sklearn.metrics import make_scorer, recall_score, accuracy_score, f1_score, precision_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import ttest_rel\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from processamento import df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b18056",
   "metadata": {},
   "source": [
    "## 4.3.1 Novo atributo: RespDisease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53d99e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asthma: 10254\n",
      "Chronic obstructive pulmonary disease: 8485\n",
      "Doenças respiratórias: 18739\n"
     ]
    }
   ],
   "source": [
    "# Definir as doenças respiratórias\n",
    "respiratory_conditions = ['Asthma', 'Chronic obstructive pulmonary disease']\n",
    "\n",
    "# Criar o novo atributo RespDisease: 1 se for doença respiratória, 0 caso contrário\n",
    "df['RespDisease'] = df['Outcome'].apply(lambda x: 1 if x in respiratory_conditions else 0)\n",
    "\n",
    "# Obter dataframe apenas com valores númericos\n",
    "df_num = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Contagem específica\n",
    "asthma_count = df['Outcome'].value_counts().get('Asthma', 0)\n",
    "copd_count = df['Outcome'].value_counts().get('Chronic obstructive pulmonary disease', 0)\n",
    "\n",
    "print(f\"Asthma: {asthma_count}\")\n",
    "print(f\"Chronic obstructive pulmonary disease: {copd_count}\")\n",
    "\n",
    "# Confirmar com RespDisease\n",
    "print(f\"Doenças respiratórias: {(df['RespDisease'] == 1).sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2378198",
   "metadata": {},
   "source": [
    "# **4.3.2 Modelos de previsão de doenças respiratórias**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f26e0d2",
   "metadata": {},
   "source": [
    "### **Nota**: A 4.3.2 está realizada juntamente com a 4.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c411483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a função para calcular a especificidade\n",
    "# A especificidade mede a proporção de negativos corretamente identificados (i.e., verdadeiros negativos)\n",
    "# Como o recall_score assume por padrão o rótulo positivo como 1, aqui define pos_label=0 \n",
    "# para que o cálculo seja feito sobre a classe negativa (ausência de doença)\n",
    "def specificity_score(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred, pos_label=0)\n",
    "\n",
    "# Cria um scorer customizado compatível com funções de validação, como cross_validate e GridSearchCV\n",
    "specificity_scorer = make_scorer(specificity_score)\n",
    "\n",
    "# Define as métricas a serem calculadas durante a validação dos modelos\n",
    "# - Accuracy: Proporção total de classificações corretas\n",
    "# - Sensitivity (Recall): Capacidade de identificar corretamente os positivos (doentes)\n",
    "# - Specificity: Capacidade de identificar corretamente os negativos (não-doentes)\n",
    "# - F1: Média harmónica entre precisão e recall (útil para classes desbalanceadas)\n",
    "scoring = {\n",
    "    'Accuracy': 'accuracy',\n",
    "    'Sensitivity': 'recall',\n",
    "    'Specificity': specificity_scorer,\n",
    "    'F1': 'f1'\n",
    "}\n",
    "\n",
    "# Define a matriz de atributos (X) e o vetor alvo (y)\n",
    "# - Remove a variável alvo RespDisease de X, para não ser usada como input\n",
    "# - y é a variável binária indicando presença ou ausência de doenças respiratórias\n",
    "X = df_num.drop(columns=['RespDisease'])\n",
    "y = df_num['RespDisease']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f03dcd",
   "metadata": {},
   "source": [
    "> * Para todos os modelos, foi utilizada uma validação cruzada com otimização dos hiperparâmetros través do método GridSearchCV dividida em 5 partições (5-fold cross-validation). \n",
    "> \n",
    "> * Escolheu-se a validação cruzada com 5 partições, por esta ser bastante utilizada, permite estimar o desempenho dos modelos de forma fiável e mantem a eficiência computacional.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aa003f",
   "metadata": {},
   "source": [
    "#### a. Modelo árvore de decisão"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1af916f",
   "metadata": {},
   "source": [
    "#### Testes Manuais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3291b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teste manual 1:\n",
      "\n",
      "max_depth=None, min_samples_split=2\n",
      "Accuracy: Mean = 0.798 Std = 0.004\n",
      "Precision: Mean = 0.743 Std = 0.011\n",
      "Recall: Mean = 0.719 Std = 0.006\n",
      "F1 Score: Mean = 0.731 Std = 0.005\n",
      "\n",
      "Teste manual 2:\n",
      "\n",
      "max_depth=5, min_samples_split=2\n",
      "Accuracy: Mean = 0.776 Std = 0.005\n",
      "Precision: Mean = 0.869 Std = 0.004\n",
      "Recall: Mean = 0.485 Std = 0.008\n",
      "F1 Score: Mean = 0.623 Std = 0.007\n",
      "\n",
      "Teste manual 3:\n",
      "\n",
      "max_depth=10, min_samples_split=5\n",
      "Accuracy: Mean = 0.797 Std = 0.006\n",
      "Precision: Mean = 0.915 Std = 0.021\n",
      "Recall: Mean = 0.516 Std = 0.007\n",
      "F1 Score: Mean = 0.659 Std = 0.006\n"
     ]
    }
   ],
   "source": [
    "# Divide os dados em 5 conjuntos (folds), com os dados baralhados antes.\n",
    "# O parâmetro random_state assegura que a divisão é reprodutível (mesma em cada execução).\n",
    "# Usado para criar os folds de validação cruzada\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Função para executar a validação cruzada com os hiperparâmetros (max_depth e min_samples_split) definidos\n",
    "def run_cv(max_depth, min_samples_split):\n",
    "    # Listas para armazenar os resultados das métricas em cada fold\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # Percorre os 5 folds da validação cruzada\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        # Divide os dados em conjuntos de treino e validação\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # Cria e treina o modelo de árvore de decisão com os hiperparâmetros definidos\n",
    "        model = DecisionTreeClassifier(\n",
    "            max_depth=max_depth,                     # Profundidade máxima da árvore\n",
    "            min_samples_split=min_samples_split,     # Número mínimo de amostras necessárias para dividir um nó\n",
    "            random_state=42,                         # Garantir consistência nos resultados\n",
    "        )\n",
    "        \n",
    "        # Treina o modelo com os dados de treino\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Faz previsões com os dados de validação\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        # Calcula as métricas de desempenho e guarda os resultados\n",
    "        accuracy_scores.append(accuracy_score(y_val, y_pred))\n",
    "        precision_scores.append(precision_score(y_val, y_pred, zero_division=0))\n",
    "        recall_scores.append(recall_score(y_val, y_pred, zero_division=0))\n",
    "        f1_scores.append(f1_score(y_val, y_pred, zero_division=0))\n",
    "\n",
    "    # Mostra os resultados da média e desvio padrão das métricas\n",
    "    print(f\"\\nmax_depth={max_depth}, min_samples_split={min_samples_split}\")\n",
    "    print(f\"Accuracy: Mean = {np.mean(accuracy_scores):.3f} Std = {np.std(accuracy_scores):.3f}\")\n",
    "    print(f\"Precision: Mean = {np.mean(precision_scores):.3f} Std = {np.std(precision_scores):.3f}\")\n",
    "    print(f\"Recall: Mean = {np.mean(recall_scores):.3f} Std = {np.std(recall_scores):.3f}\")\n",
    "    print(f\"F1 Score: Mean = {np.mean(f1_scores):.3f} Std = {np.std(f1_scores):.3f}\")\n",
    "\n",
    "# Teste manual 1\n",
    "print(\"Teste manual 1:\")\n",
    "run_cv(max_depth=None, min_samples_split=2)\n",
    "\n",
    "# Teste manual 2\n",
    "print()\n",
    "print(\"Teste manual 2:\")\n",
    "run_cv(max_depth=5, min_samples_split=2)\n",
    "\n",
    "# Teste manual 3\n",
    "print()\n",
    "print(\"Teste manual 3:\")\n",
    "run_cv(max_depth=10, min_samples_split=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e64df3",
   "metadata": {},
   "source": [
    "> * O teste de max_depth=None e min_samples_split=2 apresentou o melhor equilíbrio entre precisão e sensibilidade, resultando no maior F1 Score.\n",
    ">  \n",
    "> * Apesar de modelos mais simples serem mais precisos, sacrificam a capacidade de identificar corretamente os casos positivos. Assim, a árvore mais complexa revelou-se mais eficaz para este problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69f6fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Decision Tree Parameters: {'max_depth': 10, 'min_samples_split': 5}\n",
      "Best Score: 0.22\n",
      "Decision Tree Evaluation:\n",
      "Accuracy: Mean = 0.782, Std = 0.024\n",
      "Sensitivity: Mean = 0.510, Std = 0.049\n",
      "Specificity: Mean = 0.949, Std = 0.035\n",
      "F1: Mean = 0.640, Std = 0.042\n"
     ]
    }
   ],
   "source": [
    "# Hiperparâmetros do árvore de decisão\n",
    "dt_model_params = {\n",
    "    'max_depth': [None, 5, 10, 20], \n",
    "    'min_samples_split': [2, 5, 10] \n",
    "}\n",
    "\n",
    "# Otimizar hiperparâmetros do DecisionTreeClassifier\n",
    "grid = GridSearchCV(\n",
    "    DecisionTreeClassifier(),\n",
    "    dt_model_params,\n",
    "    cv=5,                                # 5-fold cross-validation\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Ajustar (treinar) o modelo com os dados\n",
    "grid.fit(X, y)\n",
    "\n",
    "# Obter os melhores parâmetros e a melhor pontuação\n",
    "dt_best_params = grid.best_params_\n",
    "dt_best_score = -grid.best_score_.round(2)\n",
    "print(f\"Best Decision Tree Parameters: {dt_best_params}\")\n",
    "print(f\"Best Score: {dt_best_score}\")\n",
    "\n",
    "# Treinar o modelo com os melhores parâmetros\n",
    "dt_model = DecisionTreeClassifier(**dt_best_params)\n",
    "\n",
    "# Avaliar desempenho do modelo \n",
    "scores = cross_validate(dt_model, X, y, cv=5, scoring=scoring, n_jobs=-1)\n",
    "\n",
    "# Calcular as métricas de avaliação\n",
    "dt_metrics = {}\n",
    "for metric in scoring.keys():\n",
    "    dt_metrics[metric] = (\n",
    "        np.mean(scores[f'test_{metric}']), \n",
    "        np.std(scores[f'test_{metric}'])\n",
    "    )\n",
    "\n",
    "# Exibir as métricas de avaliação do modelo\n",
    "print(\"Decision Tree Evaluation:\")\n",
    "for metric, (mean, std) in dt_metrics.items():\n",
    "    print(f\"{metric}: Mean = {mean:.3f}, Std = {std:.3f}\")\n",
    "\n",
    "# Guarda o modelo \n",
    "decision_tree = dt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eefb96",
   "metadata": {},
   "source": [
    "> * Melhores hiperparâmetros identificados foram max_depth=10 e min_samples_split=5;\n",
    "> \n",
    "> * Obteve-se um erro médio absoluto (MAE) de 0.22, o que indica que, em média, o desvio entre as previsões do modelo e os valores reais da variável alvo foi de 22%.\n",
    "> \n",
    "> * Este valor mostra um desempenho aceitável considerando a complexidade do fenómeno estudado e as possíveis limitações dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5775106c",
   "metadata": {},
   "source": [
    "#### b. Modelo rede neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd319e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Teste manual 1:\n",
      "\n",
      "hidden_layer_sizes=(50,), learning_rate_init=0.001\n",
      "Accuracy: Mean = 0.772 Std = 0.005\n",
      "Precision: Mean = 0.870 Std = 0.038\n",
      "Recall: Mean = 0.474 Std = 0.026\n",
      "F1 Score: Mean = 0.613 Std = 0.013\n",
      "\n",
      "Teste manual 2:\n",
      "\n",
      "hidden_layer_sizes=(100,), learning_rate_init=0.001\n",
      "Accuracy: Mean = 0.776 Std = 0.004\n",
      "Precision: Mean = 0.901 Std = 0.033\n",
      "Recall: Mean = 0.465 Std = 0.030\n",
      "F1 Score: Mean = 0.612 Std = 0.019\n",
      "\n",
      "Teste manual 3:\n",
      "\n",
      "hidden_layer_sizes=(50, 50), learning_rate_init=0.001\n",
      "Accuracy: Mean = 0.784 Std = 0.006\n",
      "Precision: Mean = 0.859 Std = 0.047\n",
      "Recall: Mean = 0.525 Std = 0.047\n",
      "F1 Score: Mean = 0.649 Std = 0.025\n"
     ]
    }
   ],
   "source": [
    "# Divide os dados em 5 conjuntos (folds), com os dados baralhados antes.\n",
    "# O parâmetro random_state assegura que a divisão é reprodutível (mesma em cada execução).\n",
    "# Usado para criar os folds de validação cruzada\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Função para executar a validação cruzada com os hiperparâmetros (n_neighbors e weights) definidos\n",
    "def run_nn_cv(hidden_layer_sizes, learning_rate_init):\n",
    "    # Listas para armazenar os resultados das métricas em cada fold\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # Percorre os 5 folds da validação cruzada\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        # Divide os dados em conjuntos de treino e validação\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # Cria e treina o modelo de rede neural com os hiperparâmetros definidos\n",
    "        model = make_pipeline(\n",
    "            # Normaliza os dados antes de treinar a rede neural\n",
    "            StandardScaler(),\n",
    "            # Cria a rede neural com os hiperparâmetros especificados\n",
    "            MLPClassifier(\n",
    "                hidden_layer_sizes=hidden_layer_sizes,  # Tamanho das camadas ocultas\n",
    "                learning_rate_init=learning_rate_init,  # Taxa de aprendizado inicial\n",
    "                max_iter=4000,                          # Número máximo de iterações para o treinamento\n",
    "                random_state=42                         # Garantir consistência nos resultados\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Treina o modelo com os dados de treino\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Faz previsões com os dados de validação\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        # Calcula as métricas de desempenho e guarda os resultados\n",
    "        accuracy_scores.append(accuracy_score(y_val, y_pred))\n",
    "        precision_scores.append(precision_score(y_val, y_pred, zero_division=0))\n",
    "        recall_scores.append(recall_score(y_val, y_pred, zero_division=0))\n",
    "        f1_scores.append(f1_score(y_val, y_pred, zero_division=0))\n",
    "\n",
    "    # Mostra os resultados da média e desvio padrão das métricas\n",
    "    print(f\"\\nhidden_layer_sizes={hidden_layer_sizes}, learning_rate_init={learning_rate_init}\")\n",
    "    print(f\"Accuracy: Mean = {np.mean(accuracy_scores):.3f} Std = {np.std(accuracy_scores):.3f}\")\n",
    "    print(f\"Precision: Mean = {np.mean(precision_scores):.3f} Std = {np.std(precision_scores):.3f}\")\n",
    "    print(f\"Recall: Mean = {np.mean(recall_scores):.3f} Std = {np.std(recall_scores):.3f}\")\n",
    "    print(f\"F1 Score: Mean = {np.mean(f1_scores):.3f} Std = {np.std(f1_scores):.3f}\")\n",
    "\n",
    "# Teste manual 1\n",
    "print()\n",
    "print(\"Teste manual 1:\")\n",
    "run_nn_cv(hidden_layer_sizes=(50,), learning_rate_init=0.001)\n",
    "\n",
    "# Teste manual 2\n",
    "print()\n",
    "print(\"Teste manual 2:\")\n",
    "run_nn_cv(hidden_layer_sizes=(100,), learning_rate_init=0.001)\n",
    "\n",
    "# Teste manual 3\n",
    "print()\n",
    "print(\"Teste manual 3:\")\n",
    "run_nn_cv(hidden_layer_sizes=(50, 50), learning_rate_init=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a261aeec",
   "metadata": {},
   "source": [
    "> * Observou-se que a arquitetura com duas camadas ocultas de 50 neurónios cada (hidden_layer_sizes=(50, 50)) e taxa de aprendizagem de 0.001 apresentou melhores resultados nas métricas de desempenho, especialmente na F1 Score.\n",
    ">  \n",
    "> * Isto reflete um bom equilíbrio entre precisão e sensibilidade, sugerindo que uma arquitetura um pouco mais complexa contribui positivamente para a capacidade preditiva do modelo neste problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59084412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Neural Network Parameters: {'mlpclassifier__hidden_layer_sizes': (50, 50), 'mlpclassifier__learning_rate_init': 0.01}\n",
      "Best Score: 0.22\n",
      "Neural Network Evaluation:\n",
      "Accuracy: Mean = 0.757, Std = 0.018\n",
      "Sensitivity: Mean = 0.456, Std = 0.098\n",
      "Specificity: Mean = 0.942, Std = 0.038\n",
      "F1: Mean = 0.581, Std = 0.068\n"
     ]
    }
   ],
   "source": [
    "nn_model = make_pipeline(StandardScaler(), MLPClassifier(max_iter=4000))\n",
    "\n",
    "# Hiperparâmetros do modelo de rede neural\n",
    "nn_model_params = {\n",
    "    'mlpclassifier__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "    'mlpclassifier__learning_rate_init': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Otimizar hiperparâmetros do MLPClassifier\n",
    "grid = GridSearchCV(nn_model, nn_model_params, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "\n",
    "# Ajustar (treinar) o modelo com os dados\n",
    "grid.fit(X, y)\n",
    "\n",
    "# Obter os melhores parâmetros e a melhor pontuação\n",
    "nn_best_params = grid.best_params_\n",
    "nn_best_score = -grid.best_score_.round(2)\n",
    "print(f\"Best Neural Network Parameters: {nn_best_params}\")\n",
    "print(f\"Best Score: {dt_best_score}\")\n",
    "\n",
    "# Treinar o modelo com os melhores parâmetros\n",
    "nn_model.set_params(**nn_best_params)\n",
    "\n",
    "# Avaliar desempenho do modelo \n",
    "scores = cross_validate(nn_model, X, y, cv=5, scoring=scoring, n_jobs=-1)\n",
    "\n",
    "# Calcular as métricas de avaliação\n",
    "nn_metrics = {}\n",
    "for metric in scoring.keys():\n",
    "    nn_metrics[metric] = (np.mean(scores[f'test_{metric}']), np.std(scores[f'test_{metric}']))\n",
    "\n",
    "# Exibir as métricas de avaliação do modelo\n",
    "print(\"Neural Network Evaluation:\")\n",
    "for metric, (mean, std) in nn_metrics.items():\n",
    "    print(f\"{metric}: Mean = {mean:.3f}, Std = {std:.3f}\")\n",
    "\n",
    "neural_network = nn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715cb8f4",
   "metadata": {},
   "source": [
    "> * Melhor configuração a rede com camadas ocultas de tamanho (50, 50) e uma taxa de aprendizagem inicial de 0.01;\n",
    ">\n",
    "> * MAE de 0.23, valor idêntico ao obtido pela árvore de decisão otimizada anteriormente;\n",
    ">\n",
    "> * Este resultado sugere que, em termos médios, o modelo neural apresenta um desempenho semelhante ao modelo mais simples, apesar da sua maior complexidade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67eed5d",
   "metadata": {},
   "source": [
    "#### c. Modelo SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ac3fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teste manual 1:\n",
      "\n",
      "Kernel: linear\n",
      "Accuracy: Mean = 0.691 Std = 0.004\n",
      "Precision: Mean = 0.996 Std = 0.002\n",
      "Recall: Mean = 0.189 Std = 0.005\n",
      "F1 Score: Mean = 0.318 Std = 0.007\n",
      "\n",
      "Teste manual 2:\n",
      "\n",
      "Kernel: rbf\n",
      "Accuracy: Mean = 0.691 Std = 0.004\n",
      "Precision: Mean = 1.000 Std = 0.001\n",
      "Recall: Mean = 0.189 Std = 0.005\n",
      "F1 Score: Mean = 0.318 Std = 0.007\n"
     ]
    }
   ],
   "source": [
    "# Divide os dados em 5 conjuntos (folds), com os dados baralhados antes\n",
    "# O parâmetro random_state assegura que a divisão é reprodutível (mesma em cada execução)\n",
    "# Usado para criar os folds de validação cruzada\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Função para executar a validação cruzada com os hiperparâmetros (kernel_type) definidos\n",
    "def run_svm_cv(kernel_type):\n",
    "    # Listas para armazenar os resultados das métricas em cada fold\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # Percorre os 5 folds da validação cruzada\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        # Divide os dados em conjuntos de treino e validação\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # Cria e treina o modelo SVM com o kernel especificado\n",
    "        model = make_pipeline(\n",
    "            StandardScaler(),\n",
    "            SVC(kernel=kernel_type, random_state=42)\n",
    "        )\n",
    "\n",
    "        # Treina o modelo com os dados de treino\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Faz previsões com os dados de validação\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        # Calcula as métricas de desempenho e guarda os resultados\n",
    "        accuracy_scores.append(accuracy_score(y_val, y_pred))\n",
    "        precision_scores.append(precision_score(y_val, y_pred, zero_division=0))\n",
    "        recall_scores.append(recall_score(y_val, y_pred, zero_division=0))\n",
    "        f1_scores.append(f1_score(y_val, y_pred, zero_division=0))\n",
    "\n",
    "    # Mostra os resultados da média e desvio padrão das métricas\n",
    "    print(f\"\\nKernel: {kernel_type}\")\n",
    "    print(f\"Accuracy: Mean = {np.mean(accuracy_scores):.3f} Std = {np.std(accuracy_scores):.3f}\")\n",
    "    print(f\"Precision: Mean = {np.mean(precision_scores):.3f} Std = {np.std(precision_scores):.3f}\")\n",
    "    print(f\"Recall: Mean = {np.mean(recall_scores):.3f} Std = {np.std(recall_scores):.3f}\")\n",
    "    print(f\"F1 Score: Mean = {np.mean(f1_scores):.3f} Std = {np.std(f1_scores):.3f}\")\n",
    "\n",
    "# Teste manual 1: kernel linear\n",
    "print(\"Teste manual 1:\")\n",
    "run_svm_cv(kernel_type='linear')\n",
    "\n",
    "# Teste manual 2: kernel RBF\n",
    "print()\n",
    "print(\"Teste manual 2:\")\n",
    "run_svm_cv(kernel_type='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2af0a7d",
   "metadata": {},
   "source": [
    "> * O modelo SVM com kernel RBF apresentou melhor desempenho, com F1 Score superior ao do kernel linear\n",
    "> \n",
    "> * Maior capacidade de modelar padrões não lineares presentes nos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bfe98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM Parameters: {'svc__kernel': 'rbf'}\n",
      "Best Score: 0.22\n",
      "SVM Evaluation:\n",
      "Accuracy: Mean = 0.691, Std = 0.015\n",
      "Sensitivity: Mean = 0.189, Std = 0.040\n",
      "Specificity: Mean = 1.000, Std = 0.000\n",
      "F1: Mean = 0.316, Std = 0.059\n"
     ]
    }
   ],
   "source": [
    "svm_model = make_pipeline(StandardScaler(), SVC())\n",
    "\n",
    "# Hiperparâmetros do modelo SVM\n",
    "svm_model_params = {\n",
    "    'svc__kernel': ['linear', 'rbf'],\n",
    "}\n",
    "\n",
    "# Otimizar hiperparâmetros do SVM\n",
    "grid = GridSearchCV(svm_model, svm_model_params, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "\n",
    "# Ajustar (treinar) o modelo com os dados\n",
    "grid.fit(X, y)\n",
    "\n",
    "# Obter os melhores parâmetros e a melhor pontuação\n",
    "svm_best_params = grid.best_params_\n",
    "svm_best_score = -grid.best_score_.round(2)\n",
    "print(f\"Best SVM Parameters: {svm_best_params}\")\n",
    "print(f\"Best Score: {dt_best_score}\")\n",
    "\n",
    "# Treinar o modelo com os melhores parâmetros\n",
    "svm_model.set_params(**svm_best_params)\n",
    "\n",
    "# Avaliar desempenho do modelo\n",
    "scores = cross_validate(svm_model, X, y, cv=5, scoring=scoring, n_jobs=-1)\n",
    "\n",
    "# Calcular as métricas de avaliação\n",
    "svm_metrics = {}\n",
    "for metric in scoring.keys():\n",
    "    svm_metrics[metric] = (np.mean(scores[f'test_{metric}']), np.std(scores[f'test_{metric}']))\n",
    "\n",
    "# Exibir as métricas de avaliação do modelo\n",
    "print(\"SVM Evaluation:\")\n",
    "for metric, (mean, std) in svm_metrics.items():\n",
    "    print(f\"{metric}: Mean = {mean:.3f}, Std = {std:.3f}\")\n",
    "\n",
    "svm = svm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e63e350",
   "metadata": {},
   "source": [
    "> * RBF apresenta melhor desempenho \n",
    ">\n",
    "> * Atingiu um MAE de 0.31 e este resultado indica um desempenho inferior comparado com os modelos anteriores;\n",
    "> \n",
    "> * Este resultado significa que o SVM não foi tão eficaz na previsão da variável de interesse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c446adb4",
   "metadata": {},
   "source": [
    "#### d. Modelo K-vizinhos-mais-próximos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cebbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teste manual 1:\n",
      "\n",
      "n_neighbors: 3\n",
      "Accuracy: Mean = 0.797 Std = 0.005\n",
      "Precision: Mean = 0.803 Std = 0.010\n",
      "Recall: Mean = 0.621 Std = 0.006\n",
      "F1 Score: Mean = 0.700 Std = 0.006\n",
      "\n",
      "Teste manual 2:\n",
      "\n",
      "n_neighbors: 5\n",
      "Accuracy: Mean = 0.767 Std = 0.004\n",
      "Precision: Mean = 0.793 Std = 0.011\n",
      "Recall: Mean = 0.525 Std = 0.008\n",
      "F1 Score: Mean = 0.632 Std = 0.007\n",
      "\n",
      "Teste manual 3:\n",
      "\n",
      "n_neighbors: 10\n",
      "Accuracy: Mean = 0.756 Std = 0.004\n",
      "Precision: Mean = 0.848 Std = 0.014\n",
      "Recall: Mean = 0.439 Std = 0.010\n",
      "F1 Score: Mean = 0.578 Std = 0.008\n"
     ]
    }
   ],
   "source": [
    "# Divide os dados em 5 conjuntos (folds), com os dados baralhados antes.\n",
    "# O parâmetro random_state assegura que a divisão é reprodutível (mesma em cada execução).\n",
    "# Usado para criar os folds de validação cruzada\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Função para executar a validação cruzada com os hiperparâmetros (n_neighbors) definidos\n",
    "def run_knn_cv(n_neighbors):\n",
    "    # Listas para armazenar os resultados das métricas em cada fold\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # Percorre os 5 folds da validação cruzada\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        # Divide os dados em conjuntos de treino e validação\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # Cria e treina o modelo KNN com o número de vizinhos especificado\n",
    "        model = make_pipeline(\n",
    "            StandardScaler(),\n",
    "            KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "        )\n",
    "\n",
    "        # Treina o modelo com os dados de treino\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Faz previsões com os dados de validação\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        # Faz previsões com os dados de validação\n",
    "        accuracy_scores.append(accuracy_score(y_val, y_pred))\n",
    "        precision_scores.append(precision_score(y_val, y_pred, zero_division=0))\n",
    "        recall_scores.append(recall_score(y_val, y_pred, zero_division=0))\n",
    "        f1_scores.append(f1_score(y_val, y_pred, zero_division=0))\n",
    "\n",
    "    # Mostra os resultados da média e desvio padrão das métricas\n",
    "    print(f\"\\nn_neighbors: {n_neighbors}\")\n",
    "    print(f\"Accuracy: Mean = {np.mean(accuracy_scores):.3f} Std = {np.std(accuracy_scores):.3f}\")\n",
    "    print(f\"Precision: Mean = {np.mean(precision_scores):.3f} Std = {np.std(precision_scores):.3f}\")\n",
    "    print(f\"Recall: Mean = {np.mean(recall_scores):.3f} Std = {np.std(recall_scores):.3f}\")\n",
    "    print(f\"F1 Score: Mean = {np.mean(f1_scores):.3f} Std = {np.std(f1_scores):.3f}\")\n",
    "\n",
    "# Teste manual 1: k = 3\n",
    "print(\"Teste manual 1:\")\n",
    "run_knn_cv(n_neighbors=3)\n",
    "\n",
    "# Teste manual 2: k = 5\n",
    "print()\n",
    "print(\"Teste manual 2:\")\n",
    "run_knn_cv(n_neighbors=5)\n",
    "\n",
    "# Teste manual 3: k = 10\n",
    "print()\n",
    "print(\"Teste manual 3:\")\n",
    "run_knn_cv(n_neighbors=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4ade47",
   "metadata": {},
   "source": [
    "> * O valor k=10 proporcionou o melhor equilíbrio entre precisão e estabilidade das métricas; \n",
    "> \n",
    "> * Isso indica que, para este conjunto de dados, um número maior de vizinhos permite uma melhor generalização do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea08f456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best KNN Parameters: {'kneighborsclassifier__n_neighbors': 10}\n",
      "Best Score: 0.22\n",
      "KNN Evaluation:\n",
      "Accuracy: Mean = 0.728, Std = 0.011\n",
      "Sensitivity: Mean = 0.429, Std = 0.047\n",
      "Specificity: Mean = 0.912, Std = 0.037\n",
      "F1: Mean = 0.544, Std = 0.028\n"
     ]
    }
   ],
   "source": [
    "knn_model = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "\n",
    "# Hiperparâmetros do modelo KNN\n",
    "knn_model_params = {\n",
    "    'kneighborsclassifier__n_neighbors': [3, 5, 10],\n",
    "}\n",
    "\n",
    "# Otimizar hiperparâmetros do KNeighborsClassifier\n",
    "grid = GridSearchCV(knn_model, knn_model_params, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "\n",
    "# Ajustar (treinar) o modelo com os dados\n",
    "grid.fit(X, y)\n",
    "\n",
    "# Obter os melhores parâmetros e a melhor pontuação\n",
    "knn_best_params = grid.best_params_\n",
    "knn_best_score = -grid.best_score_.round(2)\n",
    "print(f\"Best KNN Parameters: {knn_best_params}\")\n",
    "print(f\"Best Score: {dt_best_score}\")\n",
    "\n",
    "# Treinar o modelo com os melhores parâmetros\n",
    "knn_model.set_params(**knn_best_params)\n",
    "\n",
    "# Avaliar desempenho do modelo\n",
    "scores = cross_validate(knn_model, X, y, cv=5, scoring=scoring, n_jobs=-1)\n",
    "\n",
    "# Calcular as métricas de avaliação\n",
    "knn_metrics = {}\n",
    "for metric in scoring.keys():\n",
    "    knn_metrics[metric] = (np.mean(scores[f'test_{metric}']), np.std(scores[f'test_{metric}']))\n",
    "\n",
    "# Exibir as métricas de avaliação do modelo\n",
    "print(\"KNN Evaluation:\")\n",
    "for metric, (mean, std) in knn_metrics.items():\n",
    "    print(f\"{metric}: Mean = {mean:.3f}, Std = {std:.3f}\")\n",
    "\n",
    "knn = knn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b09ec5",
   "metadata": {},
   "source": [
    "> * O melhor número de vizinhos (n_neighbors) foi o 10;\n",
    ">\n",
    "> * O MAE foi de 0.27, o que significa que, apesar de ter demonstrado uma capacidade preditiva razoável, a sua simplicidade e, por ser, um método baseado em instâncias, podem comprometer a sua capacidade de generalização."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d074dc",
   "metadata": {},
   "source": [
    "# 4.3.4 Modelo com melhor desempenho entre os dois melhores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed1a1c9",
   "metadata": {},
   "source": [
    "> O teste t de Student foi escolhido, pois compara diretamente duas séries de resultados dependentes (folds iguais) e com este é possível observar as diferenças entre os modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd44dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Model A Mean</th>\n",
       "      <th>Model B Mean</th>\n",
       "      <th>t-statistic</th>\n",
       "      <th>p-value</th>\n",
       "      <th>Significant (α=0.05)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.781644</td>\n",
       "      <td>0.766585</td>\n",
       "      <td>1.956553</td>\n",
       "      <td>0.122036</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.510966</td>\n",
       "      <td>0.552645</td>\n",
       "      <td>-2.971801</td>\n",
       "      <td>0.041071</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.948488</td>\n",
       "      <td>0.898458</td>\n",
       "      <td>2.611616</td>\n",
       "      <td>0.059319</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1</td>\n",
       "      <td>0.640129</td>\n",
       "      <td>0.643073</td>\n",
       "      <td>-0.445202</td>\n",
       "      <td>0.679205</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Metric  Model A Mean  Model B Mean  t-statistic   p-value  \\\n",
       "0     Accuracy      0.781644      0.766585     1.956553  0.122036   \n",
       "1  Sensitivity      0.510966      0.552645    -2.971801  0.041071   \n",
       "2  Specificity      0.948488      0.898458     2.611616  0.059319   \n",
       "3           F1      0.640129      0.643073    -0.445202  0.679205   \n",
       "\n",
       "   Significant (α=0.05)  \n",
       "0                 False  \n",
       "1                  True  \n",
       "2                 False  \n",
       "3                 False  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definir métricas de avaliação a usar na cross_validate\n",
    "scoring = {\n",
    "    'Accuracy': 'accuracy',\n",
    "    'Sensitivity': 'recall',\n",
    "    'Specificity': specificity_scorer,\n",
    "    'F1': 'f1'\n",
    "}\n",
    "\n",
    "# Modelos a comparar\n",
    "# model_a: árvore de decisão (decision_tree)\n",
    "# model_b: rede neural (neural_network)\n",
    "model_a = decision_tree\n",
    "model_b = neural_network\n",
    "\n",
    "# Prepara as variáveis preditoras (X) e a variável alvo (y)\n",
    "# A variável RespDisease é o alvo\n",
    "X = df_num.drop(columns=['RespDisease'])\n",
    "y = df_num['RespDisease']\n",
    "\n",
    "# Avaliar desempenho dos modelos\n",
    "scores_a = cross_validate(model_a, X, y, cv=5, scoring=scoring, n_jobs=-1)\n",
    "scores_b = cross_validate(model_b, X, y, cv=5, scoring=scoring, n_jobs=-1)\n",
    "\n",
    "# Compara os desempenhos das duas abordagens para cada métrica de avaliação\n",
    "# Utiliza o teste t de Student para amostras emparelhadas (ttest_rel), pois os folds são iguais\n",
    "# Avalia se as diferenças observadas são estatisticamente significativas (nível de significância de 5%)\n",
    "results = []\n",
    "for metric in scoring.keys():\n",
    "    # Valores do modelo A (árvore)\n",
    "    a_scores = scores_a[f'test_{metric}']\n",
    "\n",
    "    # Valores do modelo B (rede neural)\n",
    "    b_scores = scores_b[f'test_{metric}']\n",
    "\n",
    "    # teste t Student\n",
    "    t_stat, p_val = ttest_rel(a_scores, b_scores)\n",
    "\n",
    "    # Diferença significativa se p < 0.05\n",
    "    significant = p_val < 0.05\n",
    "\n",
    "    results.append({\n",
    "        'Metric': metric,\n",
    "        'Model A Mean': np.mean(a_scores),\n",
    "        'Model B Mean': np.mean(b_scores),\n",
    "        't-statistic': t_stat,\n",
    "        'p-value': p_val,\n",
    "        'Significant (α=0.05)': significant\n",
    "    })\n",
    "\n",
    "# Mostrar resultados \n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea43655",
   "metadata": {},
   "source": [
    "> * Accuracy: não existiu grande diferença significativa, entre os dois modelos (p=0.122).  \n",
    "> \n",
    "> * Sensitivity: a Rede Neuronal apresentou um melhor desempenho, com uma diferença estatisticamente significativa (p=0.041), isto significa que este é melhor a identificar corretamente pacientes com doenças respiratórias.\n",
    "> \n",
    "> * Specificity: foi maior na Árvore de Decisão, significando que este pode ser mais eficaz a reconhecer negativos verdadeiros.\n",
    "> \n",
    "> * F1: não revelou diferenças relevantes entre os modelos (p=0.697), indicando um desempenho igual entre falsos positivos e falsos negativos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1652fc",
   "metadata": {},
   "source": [
    "# 4.3.5 Comparação dos resultados do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5538b6eb",
   "metadata": {},
   "source": [
    "> * Accuracy: a Árvore de Decisão teve melhor desempenho, enquanto que o SVM teve pior, ou seja, este último classificou incorretamente a maior parte das instâncias.\n",
    "> \n",
    "> * Sensitivity: a Árvore de decisão teve melhor desempenho, mas o SVM teve uma sensibilidade baixa, o que indica que ocorrem falhas na deteção de casos positivos.\n",
    "> \n",
    "> * Specificity: o SVM teve melhor desempenho, mas isto ocorreu às custas de uma sensibilidade muito baixa, ou seja, não detetou positivos; o pior foi a Rede Neuronal, apesar de ainda ser bastante alta.\n",
    ">\n",
    "> * F1: a Árvore de Decisão teve melhor desempenho, ao contrário do SVM que obteve um F1 muito baixo, o que significa um desempenho desequilibrado.\n",
    ">\n",
    "> Conclusão: o melhor modelo é a Árvore de Decisão, uma vez que teve melhor desempenho em Accuracy, Sensitivity e F1 e o pior modelo é o SVM, pois falhou na identificação de positivos, ou seja, risco de ignorar pacientes doentes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
