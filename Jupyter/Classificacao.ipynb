{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6209f0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from sklearn.metrics import make_scorer, recall_score, accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from processamento import df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "53d99e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asthma: 10254\n",
      "Chronic obstructive pulmonary disease: 8485\n",
      "Doenças respiratórias: 18739\n"
     ]
    }
   ],
   "source": [
    "# Definir as doenças respiratórias\n",
    "respiratory_conditions = ['Asthma', 'Chronic obstructive pulmonary disease']\n",
    "\n",
    "# Criar o novo atributo RespDisease: 1 se for doença respiratória, 0 caso contrário\n",
    "df['RespDisease'] = df['Outcome'].apply(lambda x: 1 if x in respiratory_conditions else 0)\n",
    "\n",
    "# Obter dataframe apenas com valores númericos\n",
    "df_num = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Contagem específica\n",
    "asthma_count = df['Outcome'].value_counts().get('Asthma', 0)\n",
    "copd_count = df['Outcome'].value_counts().get('Chronic obstructive pulmonary disease', 0)\n",
    "\n",
    "print(f\"Asthma: {asthma_count}\")\n",
    "print(f\"Chronic obstructive pulmonary disease: {copd_count}\")\n",
    "\n",
    "# Confirmar com RespDisease\n",
    "print(f\"Doenças respiratórias: {(df['RespDisease'] == 1).sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2378198",
   "metadata": {},
   "source": [
    "# **4.3.2 Modelos de previsão de doenças respiratórias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c411483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define especificidade\n",
    "def specificity_score(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred, pos_label=0)\n",
    "\n",
    "specificity_scorer = make_scorer(specificity_score)\n",
    "\n",
    "# Define métricas\n",
    "scoring = {\n",
    "    'Accuracy': 'accuracy',\n",
    "    'Sensitivity': 'recall',\n",
    "    'Specificity': specificity_scorer,\n",
    "    'F1': 'f1'\n",
    "}\n",
    "\n",
    "X = df_num.drop(columns=['RespDisease'])\n",
    "y = df_num['RespDisease']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aa003f",
   "metadata": {},
   "source": [
    "#### a. Modelo árvore de decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69f6fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Decision Tree Parameters: {'max_depth': 10, 'min_samples_split': 10}\n",
      "Best Score: 0.22\n",
      "Decision Tree Evaluation:\n",
      "Accuracy: Mean = 0.782, Std = 0.024\n",
      "Sensitivity: Mean = 0.511, Std = 0.049\n",
      "Specificity: Mean = 0.949, Std = 0.035\n",
      "F1: Mean = 0.640, Std = 0.042\n"
     ]
    }
   ],
   "source": [
    "# Hiperparâmetros do árvore de decisão\n",
    "dt_model_params = {\n",
    "    'max_depth': [None, 5, 10, 20], \n",
    "    'min_samples_split': [2, 5, 10] \n",
    "}\n",
    "\n",
    "# Otimizar hiperparâmetros do DecisionTreeClassifier\n",
    "grid = GridSearchCV(\n",
    "    DecisionTreeClassifier(),\n",
    "    dt_model_params,\n",
    "    cv=5,                                # 5-fold cross-validation\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Ajustar (treinar) o modelo com os dados\n",
    "grid.fit(X, y)\n",
    "\n",
    "# Obter os melhores parâmetros e a melhor pontuação\n",
    "dt_best_params = grid.best_params_\n",
    "dt_best_score = -grid.best_score_.round(2)\n",
    "print(f\"Best Decision Tree Parameters: {dt_best_params}\")\n",
    "print(f\"Best Score: {dt_best_score}\")\n",
    "\n",
    "# Treinar o modelo com os melhores parâmetros\n",
    "dt_model = DecisionTreeClassifier(**dt_best_params)\n",
    "\n",
    "# Avaliar desempenho do modelo \n",
    "scores = cross_validate(dt_model, X, y, cv=5, scoring=scoring, n_jobs=-1)\n",
    "\n",
    "# Calcular as métricas de avaliação\n",
    "dt_metrics = {}\n",
    "for metric in scoring.keys():\n",
    "    dt_metrics[metric] = (\n",
    "        np.mean(scores[f'test_{metric}']), \n",
    "        np.std(scores[f'test_{metric}'])\n",
    "    )\n",
    "\n",
    "# Exibir as métricas de avaliação do modelo\n",
    "print(\"Decision Tree Evaluation:\")\n",
    "for metric, (mean, std) in dt_metrics.items():\n",
    "    print(f\"{metric}: Mean = {mean:.3f}, Std = {std:.3f}\")\n",
    "\n",
    "# Guarda o modelo \n",
    "decision_tree = dt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5775106c",
   "metadata": {},
   "source": [
    "#### b. Modelo rede neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59084412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Neural Network Parameters: {'mlpclassifier__hidden_layer_sizes': (50, 50), 'mlpclassifier__learning_rate_init': 0.01}\n",
      "Best Score: 0.22\n",
      "Neural Network Evaluation:\n",
      "Accuracy: Mean = 0.757, Std = 0.018\n",
      "Sensitivity: Mean = 0.456, Std = 0.098\n",
      "Specificity: Mean = 0.942, Std = 0.038\n",
      "F1: Mean = 0.581, Std = 0.068\n"
     ]
    }
   ],
   "source": [
    "nn_model = make_pipeline(StandardScaler(), MLPClassifier(max_iter=4000))\n",
    "\n",
    "# Hiperparâmetros do modelo de rede neural\n",
    "nn_model_params = {\n",
    "    'mlpclassifier__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "    'mlpclassifier__learning_rate_init': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Otimizar hiperparâmetros do MLPClassifier\n",
    "grid = GridSearchCV(nn_model, nn_model_params, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "\n",
    "# Ajustar (treinar) o modelo com os dados\n",
    "grid.fit(X, y)\n",
    "\n",
    "# Obter os melhores parâmetros e a melhor pontuação\n",
    "nn_best_params = grid.best_params_\n",
    "nn_best_score = -grid.best_score_.round(2)\n",
    "print(f\"Best Neural Network Parameters: {nn_best_params}\")\n",
    "print(f\"Best Score: {dt_best_score}\")\n",
    "\n",
    "# Treinar o modelo com os melhores parâmetros\n",
    "nn_model.set_params(**nn_best_params)\n",
    "\n",
    "# Avaliar desempenho do modelo \n",
    "scores = cross_validate(nn_model, X, y, cv=5, scoring=scoring, n_jobs=-1)\n",
    "\n",
    "# Calcular as métricas de avaliação\n",
    "nn_metrics = {}\n",
    "for metric in scoring.keys():\n",
    "    nn_metrics[metric] = (np.mean(scores[f'test_{metric}']), np.std(scores[f'test_{metric}']))\n",
    "\n",
    "# Exibir as métricas de avaliação do modelo\n",
    "print(\"Neural Network Evaluation:\")\n",
    "for metric, (mean, std) in nn_metrics.items():\n",
    "    print(f\"{metric}: Mean = {mean:.3f}, Std = {std:.3f}\")\n",
    "\n",
    "neural_network = nn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67eed5d",
   "metadata": {},
   "source": [
    "#### c. Modelo SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bfe98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM Parameters: {'svc__kernel': 'rbf'}\n",
      "Best Score: 0.22\n",
      "SVM Evaluation:\n",
      "Accuracy: Mean = 0.691, Std = 0.015\n",
      "Sensitivity: Mean = 0.189, Std = 0.040\n",
      "Specificity: Mean = 1.000, Std = 0.000\n",
      "F1: Mean = 0.316, Std = 0.059\n"
     ]
    }
   ],
   "source": [
    "svm_model = make_pipeline(StandardScaler(), SVC())\n",
    "\n",
    "# Hiperparâmetros do modelo SVM\n",
    "svm_model_params = {\n",
    "    'svc__kernel': ['linear', 'rbf'],\n",
    "}\n",
    "\n",
    "# Otimizar hiperparâmetros do SVM\n",
    "grid = GridSearchCV(svm_model, svm_model_params, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "\n",
    "# Ajustar (treinar) o modelo com os dados\n",
    "grid.fit(X, y)\n",
    "\n",
    "# Obter os melhores parâmetros e a melhor pontuação\n",
    "svm_best_params = grid.best_params_\n",
    "svm_best_score = -grid.best_score_.round(2)\n",
    "print(f\"Best SVM Parameters: {svm_best_params}\")\n",
    "print(f\"Best Score: {dt_best_score}\")\n",
    "\n",
    "# Treinar o modelo com os melhores parâmetros\n",
    "svm_model.set_params(**svm_best_params)\n",
    "\n",
    "# Avaliar desempenho do modelo\n",
    "scores = cross_validate(svm_model, X, y, cv=5, scoring=scoring, n_jobs=-1)\n",
    "\n",
    "# Calcular as métricas de avaliação\n",
    "svm_metrics = {}\n",
    "for metric in scoring.keys():\n",
    "    svm_metrics[metric] = (np.mean(scores[f'test_{metric}']), np.std(scores[f'test_{metric}']))\n",
    "\n",
    "# Exibir as métricas de avaliação do modelo\n",
    "print(\"SVM Evaluation:\")\n",
    "for metric, (mean, std) in svm_metrics.items():\n",
    "    print(f\"{metric}: Mean = {mean:.3f}, Std = {std:.3f}\")\n",
    "\n",
    "svm = svm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c446adb4",
   "metadata": {},
   "source": [
    "#### d. Modelo K-vizinhos-mais-próximos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea08f456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best KNN Parameters: {'kneighborsclassifier__n_neighbors': 10}\n",
      "Best Score: 0.22\n",
      "KNN Evaluation:\n",
      "Accuracy: Mean = 0.728, Std = 0.011\n",
      "Sensitivity: Mean = 0.429, Std = 0.047\n",
      "Specificity: Mean = 0.912, Std = 0.037\n",
      "F1: Mean = 0.544, Std = 0.028\n"
     ]
    }
   ],
   "source": [
    "knn_model = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "\n",
    "# Hiperparâmetros do modelo KNN\n",
    "knn_model_params = {\n",
    "    'kneighborsclassifier__n_neighbors': [3, 5, 10],\n",
    "}\n",
    "\n",
    "# Otimizar hiperparâmetros do KNeighborsClassifier\n",
    "grid = GridSearchCV(knn_model, knn_model_params, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "\n",
    "# Ajustar (treinar) o modelo com os dados\n",
    "grid.fit(X, y)\n",
    "\n",
    "# Obter os melhores parâmetros e a melhor pontuação\n",
    "knn_best_params = grid.best_params_\n",
    "knn_best_score = -grid.best_score_.round(2)\n",
    "print(f\"Best KNN Parameters: {knn_best_params}\")\n",
    "print(f\"Best Score: {dt_best_score}\")\n",
    "\n",
    "# Treinar o modelo com os melhores parâmetros\n",
    "knn_model.set_params(**knn_best_params)\n",
    "\n",
    "# Avaliar desempenho do modelo\n",
    "scores = cross_validate(knn_model, X, y, cv=5, scoring=scoring, n_jobs=-1)\n",
    "\n",
    "# Calcular as métricas de avaliação\n",
    "knn_metrics = {}\n",
    "for metric in scoring.keys():\n",
    "    knn_metrics[metric] = (np.mean(scores[f'test_{metric}']), np.std(scores[f'test_{metric}']))\n",
    "\n",
    "# Exibir as métricas de avaliação do modelo\n",
    "print(\"KNN Evaluation:\")\n",
    "for metric, (mean, std) in knn_metrics.items():\n",
    "    print(f\"{metric}: Mean = {mean:.3f}, Std = {std:.3f}\")\n",
    "\n",
    "knn = knn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd44dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Model A Mean</th>\n",
       "      <th>Model B Mean</th>\n",
       "      <th>t-statistic</th>\n",
       "      <th>p-value</th>\n",
       "      <th>Significant (α=0.05)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.781644</td>\n",
       "      <td>0.766585</td>\n",
       "      <td>1.956553</td>\n",
       "      <td>0.122036</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.510966</td>\n",
       "      <td>0.552645</td>\n",
       "      <td>-2.971801</td>\n",
       "      <td>0.041071</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.948488</td>\n",
       "      <td>0.898458</td>\n",
       "      <td>2.611616</td>\n",
       "      <td>0.059319</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1</td>\n",
       "      <td>0.640129</td>\n",
       "      <td>0.643073</td>\n",
       "      <td>-0.445202</td>\n",
       "      <td>0.679205</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Metric  Model A Mean  Model B Mean  t-statistic   p-value  \\\n",
       "0     Accuracy      0.781644      0.766585     1.956553  0.122036   \n",
       "1  Sensitivity      0.510966      0.552645    -2.971801  0.041071   \n",
       "2  Specificity      0.948488      0.898458     2.611616  0.059319   \n",
       "3           F1      0.640129      0.643073    -0.445202  0.679205   \n",
       "\n",
       "   Significant (α=0.05)  \n",
       "0                 False  \n",
       "1                  True  \n",
       "2                 False  \n",
       "3                 False  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definir métricas de avaliação a usar na cross_validate\n",
    "scoring = {\n",
    "    'Accuracy': 'accuracy',\n",
    "    'Sensitivity': 'recall',\n",
    "    'Specificity': specificity_scorer,\n",
    "    'F1': 'f1'\n",
    "}\n",
    "\n",
    "# Modelos a comparar\n",
    "model_a = decision_tree\n",
    "model_b = neural_network\n",
    "\n",
    "# Prepara X e y\n",
    "X = df_num.drop(columns=['RespDisease'])\n",
    "y = df_num['RespDisease']\n",
    "\n",
    "# Avaliar desempenho dos modelos\n",
    "scores_a = cross_validate(model_a, X, y, cv=5, scoring=scoring, n_jobs=-1)\n",
    "scores_b = cross_validate(model_b, X, y, cv=5, scoring=scoring, n_jobs=-1)\n",
    "\n",
    "# Comparar os modelos usando o teste t Student para cada métrica\n",
    "results = []\n",
    "for metric in scoring.keys():\n",
    "    a_scores = scores_a[f'test_{metric}']\n",
    "    b_scores = scores_b[f'test_{metric}']\n",
    "    t_stat, p_val = ttest_rel(a_scores, b_scores)\n",
    "    significant = p_val < 0.05\n",
    "    results.append({\n",
    "        'Metric': metric,\n",
    "        'Model A Mean': np.mean(a_scores),\n",
    "        'Model B Mean': np.mean(b_scores),\n",
    "        't-statistic': t_stat,\n",
    "        'p-value': p_val,\n",
    "        'Significant (α=0.05)': significant\n",
    "    })\n",
    "\n",
    "# Mostrar resultados \n",
    "pd.DataFrame(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
